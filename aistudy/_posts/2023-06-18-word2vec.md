---
layout: post
title: 논문 리뷰 - Word2vec (1)
description: >
    Efficient Estimation of Word Representations in Vector Space
hide_description: false
category: aistudy
image:
  path: https://github.com/jungsiroo/jungsiroo.github.io/assets/54366260/3eb28061-f8d9-42cd-9174-a8a61c51ef7f
---

**해당 썸네일은 `Wonkook Lee` 님이 만드신 [`Thumbnail-Maker`](https://wonkooklee.github.io/thumbnail_maker/){:target="_blank"} 를 이용하였습니다**
{:.figcaption}

추천 시스템에서 DNN 을 활용하는 방법은 각 content 혹은 user 의 representation vector 를 이용하여 유사도를 통해 추천하는 방식이 활발하다. 해당 기법의 근간은 바로 `word2vec` 에 있다. 
기본을 알고 넘어가야 한다는 생각에 이 논문을 읽고 실제 구현까지 해볼 계획이다. 차근차근 deep하게 읽어보자. 

* this unordered seed list will be replaced by the toc
{:toc}

# 🏎️ Abstract

* 방대한 데이터셋에 있는 단어들의 `continuous vector representations` 를 계산하기 위해 새로운 두 가지 모델을 소개함.
* 단어 유사도 태스크에서 측정된 해당 representation 의 품질은 이전에 최고의 성능을 낸 다양한 `Neural Networks` 들과 비교함. 
* accuracy 측면에서 상당한 발전이 있었고 더 적은 컴퓨팅 자원을 이용함.
* 자신들이 만든 의미론적 / 문법적 단어 유사도를 측정할 수 있는 test set 에서 SOTA 성능을 냈음.

> * syntactic(문법적) 유사도
> 
>   * big-bigger-biggest / small-smaller-smallest 처럼 문법적인 유사도 유추
>
> * sementic(의미론적) 유사도
>
>   * Seoul 과 Korea 는 의미간에 유사도가 존재
{:.lead}

# 📀 Introduction

* 근래의(2013년 당시) NLP 시스템은 단어를 `원자 단위`로 취급했음
    * vocabulary 안에서 인덱스로 표현이 됨
    * 간편성, 강건함이 이것의 장점
        * 단어를 One-Hot 벡터로 표현한다면 `Encoding 과 Decoding 이 1:1 매핑`이 되기에 강건함이라고 표현한 것 같음
        * 반면 continuous 한 벡터로 표현된다면 100% 일치하는 복구가 어려움
    * 많은 양의 데이터를 학습한 간단한 모델이 적은 양의 데이터로 학습한 복잡한 모델보다 성능이 좋음


`하지만 단어를 단순히 인덱스(One hot)로 표현하는 것은 당연히 많은 제한점이 존재함`

* 단어 간 연관성 표현 불가
* ASR 분야나 기계 번역에서 성능 제한 → 데이터의 양에 의존적

<br>

머신러닝 기술의 발전으로 이제는 큰 데이터를 이용해 복잡한 모델을 학습시키는 것이 가능해짐. 통계학 기반의 N-gram 모델보다 NN 기반 LM이 비약적인 성능을 내기에
이를 이용해 `distributed representations` 을 이용하려고 함.

> * distributed representation(분산 표현) 이란?
> 
>   * 분포 가설에 기반해 주변 단어 분포 기준으로 단어의 벡터 표현이 결정되는 것
>   * One hot vector 보다 저차원이지만 dense 하게 표현이 됨 (주변 단어의 분포 정보를 내포하기에)

## Goals of paper

* 메인 목표는 고품질의 단어 벡터를 학습할 수 있는 테크닉을 소개하기 위함
    * 좋은 품질의 단어 벡터를 이용하면 유사한 단어들끼리 서로 가까이 위치함뿐만 아니라 `multiple degrees of similarity` 도 가질 수 있다고 함
    * multiple degrees of similarity
        * 같은 noun 이어도 단수 / 복수 같은 형태의 차이를 가지더라도 유사함 (ex. apple / apples)
        * 하나의 단어가 여러 개의 비슷한 의미 특성을 가질 수 있음 (big-bigger-biggest)
    * embedding vector 를 이용하여 `simple algebraic 연산`이 가능함
        * vector(_"King"_) - vector(_"Man"_) + vector(_"Woman"_) = vector(_"Queen"_)
    
논문은 새로운 모델 구조를 개발하면서 `단어 간 linear regularities(앞서 보인 simple algebraic)` 를 유지하면서 accuracy 를 최대화할려고 노력함.

이를 위해 Abstract 에서 말했던 test set 을 이용하였고 높은 accuracy 로 linear reulgarities 를 학습할 수 있음을 보였음.

## Previous Work

단어를 continuous vector 로 표현하기 위한 시도는 옛날부터 있었음

* NNLM [설명](https://wikidocs.net/45609){:target="_blank"}
    * 한 개의 linear projection layer 와 non-linear hidden layer 로 구성이 됨
    * 논문이 흥미롭게 느낀 NNLM 구조는 single hidden layer 를 통해 학습된 word vector 를 얻을 수 있다는 점
    * 논문은 이 first step 에 집중하여 간단한 모델을 이용해 word vector 를 만드는 구조를 확장함


# 🧽 Model Architecture

이전에 제안된 다양한 모델들(**LSA**, **LDA** 등)도 단어들의 `continuous representations` 를 구하려고 하였음.

이 논문에서는 neural network 로 학습된 `distributed representations of words` 에 집중하고자 함. 이는 LSA 보다 월등한 성능을 보여주는 동시에
linear regularities 도 보존하는 모습을 보임. 또한 LDA 는 많은 양의 데이터에서는 컴퓨팅적으로 expensive 한 문제점이 있음

**Time Complexity _O_**

$$
\begin{align*}
& O = E \times T \times Q \newline
& E = number \, of \, epochs \newline
& T = number \, of \, words \newline
& Q = Model \, Architecture
\end{align*}
$$
{:.lead}

## NNLM

![image](https://github.com/jungsiroo/jungsiroo.github.io/assets/54366260/64015238-127f-467f-81a6-d45b96873dfe)

NNLM Structure
{:.figcaption}

* NNLM 은 4개의 층으로 이루어져 있음.
* Input Layer 에서 N개의 이전 단어들은 V(vocabulary) 크기만큼 원핫 벡터가 만들어짐
* 이후 projection layer 를 거침
    * 이 projection layer 는 은닉층과는 다르게 `가중치 행렬과의 곱셈은 이루어지지만 활성화 함수가 존재하지 않음`
    * Lookup - table : $$W_p$$ 와 계산이 이루어져 나온 vector ($$N \times D$$)

<br>

![image](https://github.com/jungsiroo/jungsiroo.github.io/assets/54366260/e89d217d-dcc1-4b13-8360-1851856b75ce)

Projection layer 계산 과정
{:.figcaption}

* 이제 각 단어들은 lookup table 을 거쳐 벡터가 나오게 되며 projection layer 에서 concat 이 된다. ($$N \times D$$)
* 만들어진 Projection layer 를 가중치 행렬 $$W_p$$ 를 곱하고 $$tanh$$ 함수를 거치게 되면서 학습을 진행
* 마지막으로 `Cross Entropy` 거치기 위해 hidden layer 에 $$W_o (H \times V)$$ 를 곱해주어 ouput layer 생성
* $$W_p$$ 의 shape : $$(N \times D) \, \times H$$

![image](https://github.com/jungsiroo/jungsiroo.github.io/assets/54366260/46c04f68-564c-4a9d-8249-8c93b798d7bb)

Final structure
{:.figcaption}

이렇게 해서 앞선 말한 $$Q$$ 의 `time complexity` 를 계산하면 아래와 같이 계산됨

$$ Q = N \times D + N \times D \times H + H \times V$$

* 위 식에서 가장 지배적인 수식은 $$N \times D \times H$$ 임.
* 원래는 $$H \times V$$ 가 가장 지배적이지만 이를 줄일 수 있는 기법이 존재
    * Avoding Normalized : 논문에서는 해당 방법을 통해 complexity 를 줄일 수 있다하지만 정확히 이해가 안감
    * Use hierarchical softmax : 본 논문은 해당 방법을 사용하였음 
        * 이 방법과 vocab 을 `Huffman binary tree` 로 구성하여 만든 모델은 $$H \times V$$ 가 요구된다함
    * 앞서 설명했듯 vocabulary 를 허프만 완전 이진 트리를 활용해 구성한다면 $$log_2(V)$$ 만큼의 output이 만들어짐
    
**NNLM 의 한계점**

> * 가장 큰 한계는 제한된 길이의 입력 
>
> * 정해진 N만큼만 참고할 수 있기에 한정된 문맥만 학습함
{:.lead}

## RNNLM

> NNLM 의 한계점을 극복하기 위해 나온 모델
{:.lead}

* 이론적으로 RNN 계열이 더 효과적으로 얕은 NN 보다 복잡한 패턴을 나타냄
* RNN 은 projection layer 는 없으며 hideen layer 가 자신과 연결되있는 특이한 구조를 가짐
* 이러한 구조는 모델이 이전의 short term memory 를 가질 수 있게 하므로 sequential 해짐

**Time Complexity of RNN (Q)**

$$Q = H \times H + H \times V$$

* word representations D 는 hidden layer H 와 똑같은 디멘션을 가지고 있기에 $$H \times H$$ 로 계산이 됨. 
* NNLM 과 마찬가지로 $$H \times V$$ 는 `hierarchical softmax` 를 활용하여 $$H \times log_2(V)$$ 까지 줄일 수 있음.

# 🎇 New Log-linear Models

본 논문은 computational complexity 를 최소화하면서 `distributed representations` 를 학습하기 위해 2개의 새로운 모델을 제시함.

* 이전 구조들을 보면 non-linear 인 hidden layer 때문에 complexity 가 올라갔었음. 
* non-linear 때문에 NN 이 매력적이긴 하나 본 논문은 이를 좀 더 간단한 모델을 통해 학습할려고 함

![image](https://github.com/jungsiroo/jungsiroo.github.io/assets/54366260/b9e21b2b-ef3f-4f39-9505-6e0065392677)

CBOW & Countinuous Skip-gram
{:.figcaption}

본 논문은 2가지 스텝을 통해 `distributed vectors` 를 학습하고자 함

> * 간단한 모델을 통해 continuous word vectors 를 학습
>   * Continuous Bag-of-Words
>   * Skip-gram
> * 그 위에 N-gram NNLM 모델 학습 
{:.lead}

## Continuous Bag-of-Words

* NNLM 과 비슷한 구조이지만 `non-linear 레이어가 삭제됨`
* Projection layer 는 모든 단어가 공유함
* 이 결과값들을 모두 모아 평균을 구하면 이것이 Projection Layer
* `단어의 순서가 영향을 끼치지 않음`

> NNLM 과 다른 점은 이전의 단어만 쓰는 것이 아닌 미래의 단어도 사용함
{:.lead}

본 논문은 이전 4개의 단어와 이후 4개의 단어를 입력으로 사용하여 가운데 단어를 맞추는 것을 criterion 으로 삼고
학습을 하였다고 한다. 

**Time Complexity of Q**

$$Q = N \times D + D \times log_2(V)$$

NNLM 과 비교했을 때 hidden layer 관련 수식이 삭제된 모습
{:.figcaption}

## Continuous Skip-gram

> CBOW 와 비슷하지만 중심 단어를 예측하는 것이 아닌 중심 단어를 이용하여 주변 단어의 classification 을 최대화하는 방법 
{:.lead}

* 현재의 단어를 continuous projection layer 와 함께 input 으로 사용
* 전후 특정 범위만큼 단어를 예측하도록 함
* 이 범위를 늘리는 것이 word vectors 의 품질을 좋게 하지만 그에 따라 complexity 가 높아짐
* 단어 사이의 거리가 멀수록 연관도가 낮아지기에 sampling을 덜 줘서 가중치를 작게함

**Time Complexity of Q**

$$Q = C \times (D + D \times log_2(V))$$

> * C 는 단어 간 최대 길이
> * [1, C) 중에서 랜덤하게 number R 을 선택하여 R 개 이전, R 개 이후를 predict
> * 중심 단어 전후로 진행하기에 총 2R word classification 이 요구됨
> * R 의 평균 기댓값은 1/C 로 구할 수 있으며 2R 번의 계산이 필요하기에 C=2R 때문에 위에 식이 유도됨

# 🏉 Result

이전의 연구들은 단어를 주면 그 와 가장 유사한 단어들을 보여줌으로써 직관적으로 이해하기 쉬운 방법으로 평가를 진행했음.

이러한 방식은 좀 더 복잡한 관계를 나타내기 힘듬

* 단어의 유사함은 다양하게 표현될 수 있음.
    * big-bigger 가 유사한 것처럼 small-smaller 가 유사
    * big-biggest 페어와 small-smallest 페어가 유사함
* 이러한 유사함은 간단한 `algebraic operations` 로 계산할 수 있음
    * ex) vector("biggest") - vector("big") + vector("small") = vector("smallest")

또한 많은 양의 데이터를 이용한다면 vector 들은 의미 차이마저 알아낼 수 있다고 함

* ex) France-Paris / Germany-Berlin

이러한 semetic relationship 을 이용한다면 NLP 의 많은 부분에 향상을 가져올 수 있다고 함.

## Task Description

![image](https://github.com/jungsiroo/jungsiroo.github.io/assets/54366260/f3f8ad1e-82e7-467f-b10a-70e97a077afd)

Test Set
{:.figcaption}


> * 5 가지의 semantic 질문과 9개의 syntactic 질문 구성
> * 질문은 2개의 스텝으로 이루어짐
>   * 비슷한 단어 페어는 수동적으로 만듬
>   * 2개의 단어쌍을 연결
> * 오로지 Single Token 단어만 활용 
> * 위 표에서 볼 수 있 듯 word pair 1 과 word pair 2 를 구성했기에 algebraic operation 이 가능해진 것 같음
> * 유의어로 예측한 것은 틀리다고 하였음
>   * 100% 일치가 불가능
>   * word vector 의 유용도가 accuracy 와 양의 상관관계를 가질 수 있는 application 이 있을 것이라 믿기 때문
{:.lead}

--- 

# ⛳ 마무리 

이후는 적은 computational complexitiy 로 풍부한 word vectors 를 얻어냈다는 것에 큰 의의가 밝히면 본 논문은 끝나게 된다. 

본 논문에서 제시한 점은 

* hidden layer 에서 큰 비중을 가지던 computational complexity 를 줄이고자 hidden layer 를 없앰
* 이러한 간단한 모델이라도 좋은 품질의 word vectors를 얻을 수 있었음.
* 연산 비용이 획기적으로 줄었기에 큰 데이터셋으로부터 high dimesional vector 임에도 좋은 품질을 얻을 수 있다는 점

하지만 word2vec 의 명확한 한계가 존재한다.

> * Out of Vocabulary 문제
>   * 학습할 때 보지 못했던 단어라면 vector 를 생성할 수 없다
> * 단어 빈도 수에 의존적
>   * 특정 단어가 적게 나왔더라면 그 단어의 vector 의 품질은 안 좋을 수 밖에 없다. 
{:.lead}

이러한 한계점을 극복하기 위해 나온 모델이 Facebook 의 `FastText` 이다. subword 를 skip-gram 수행을 통해 OOV 문제를 해결한다고 한다.

Word Embedding 에 큰 발전을 일으킨 word2vec 을 살펴보았다. 추천시스템에 이를 계승한 item2vec, song2vec 등의 근간이 되기에 이번 논문을 리뷰하였다. 

추가로 NNLM 에 대해 알게 되어서 좋은 시간이었다고 생각한다.